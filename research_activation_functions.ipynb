{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing and creating new random response activations functions to test thier ability to twart inference attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case study Inception:  This case study is inspired from \"Secure Split Learning against Property Inference, Data Reconstruction and FSA attacks\" paper by Yunlong Mao etal\n",
    "\n",
    "We set out to investigate privacy-preserving techniques in deep learning, focusing on random response activation functions.\n",
    "1) Dataset Selection:\n",
    "We chose the Fashion MNIST dataset for our experiments due to its widespread use and similarity to real-world image classification tasks.\n",
    "2) Model Architecture:\n",
    "We designed a Convolutional Neural Network (CNN) architecture suitable for the Fashion MNIST classification task.\n",
    "3) Implementation of Random Response Activation Functions:\n",
    "We implemented four different random response activation functions:\n",
    "    a) Stochastic Laplacian Activation (SLA)\n",
    "    b) Random Response ReLU (R3elu)\n",
    "    c) Randomized Swish (RSwish)\n",
    "    d) Laplacian Swish (LapSwish)\n",
    "4) Initial Experiments:\n",
    "We conducted experiments with all four activation functions to determine their performance in terms of accuracy and randomness scores.\n",
    "5) Results Analysis:\n",
    "After analyzing the results, we found that the Stochastic Laplacian Activation (SLA) performed the best among the four in balancing accuracy and randomness.\n",
    "6) Further Investigation of SLA:\n",
    "Having identified SLA as the most promising activation function, we proceeded to conduct more in-depth experiments:\n",
    "    a) Privacy-Preserving Techniques:\n",
    "    We implemented adversarial training for the SLA model to further enhance privacy. For comparison, we created a baseline model using standard ReLU activation.\n",
    "\n",
    "    b) Evaluation Metrics:\n",
    "    We implemented three key privacy evaluation metrics:\n",
    "\n",
    "        i) Property Inference Attack\n",
    "        ii) Member Inference Attack\n",
    "        iii) Data Reconstruction Attack\n",
    "\n",
    "    c) Comprehensive Experiments:\n",
    "\n",
    "        We trained and evaluated both the SLA model (with adversarial training) and the baseline ReLU model.\n",
    "        We performed each of the attack scenarios on both models to compare their privacy-preserving capabilities.\n",
    "\n",
    "7) Final Results Analysis:\n",
    "We collected and analyzed the results from all our experiments. We compared the performance of the SLA model against the baseline in terms of both accuracy and privacy metrics.\n",
    "\n",
    "8) Visualization:\n",
    "We created visualizations to clearly illustrate the comparative performance of the SLA and baseline models across all metrics.\n",
    "Findings:\n",
    "    a) We observed how SLA affected model accuracy compared to the baseline ReLU model.\n",
    "    b) We analyzed the effectiveness of SLA in defending against various privacy attacks compared to traditional methods.\n",
    "    c) We examined the trade-off between model utility (accuracy) and privacy preservation.\n",
    "\n",
    "9) Code Implementation:\n",
    "Throughout the project, we developed and refined Python code using PyTorch for:\n",
    "\n",
    "Implementing the various random response activation functions and model architectures\n",
    "Training procedures including adversarial training for SLA\n",
    "Evaluation metrics and attack simulations\n",
    "Data processing and result visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import recall_score\n",
    "import GPyOpt\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define Stochastic Laplacian Activation (SLA)\n",
    "class StochasticLaplacianActivation(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(StochasticLaplacianActivation, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.clipK(x, self.C, self.K)\n",
    "        mask = torch.rand_like(x_hat) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=x_hat.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, torch.maximum(x_hat + laplacian_noise, torch.tensor(0.0).to(x.device)), x_hat)\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale\n",
    "\n",
    "# Define R3elu\n",
    "class R3elu(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(R3elu, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.clipK(x, self.C, self.K)\n",
    "        mask = torch.rand_like(x_hat) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=x_hat.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, F.relu(x_hat + laplacian_noise), F.relu(x_hat))\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale\n",
    "\n",
    "# Define Randomized Swish (RSwish)\n",
    "class RSwish(nn.Module):\n",
    "    def __init__(self, mean=1.0, stddev=0.1):\n",
    "        super(RSwish, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * torch.sigmoid(x)\n",
    "        noise = torch.normal(self.mean, self.stddev, size=swish.size()).to(x.device)\n",
    "        return swish * noise\n",
    "\n",
    "# Define Laplacian Swish (LapSwish)\n",
    "class LapSwish(nn.Module):\n",
    "    def __init__(self, scale=0.1, epsilon_p=1.0):\n",
    "        super(LapSwish, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.epsilon_p = epsilon_p\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * torch.sigmoid(x)\n",
    "        noise_scale = self.scale / self.epsilon_p\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=swish.size()), dtype=torch.float32).to(x.device)\n",
    "        return swish + laplacian_noise\n",
    "\n",
    "# Define the Fashion MNIST model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self, activation='SLA', **kwargs):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        if activation == 'SLA':\n",
    "            self.custom_activation = StochasticLaplacianActivation(**kwargs)\n",
    "        elif activation == 'R3elu':\n",
    "            self.custom_activation = R3elu(**kwargs)\n",
    "        elif activation == 'RSwish':\n",
    "            self.custom_activation = RSwish(**kwargs)\n",
    "        elif activation == 'LapSwish':\n",
    "            self.custom_activation = LapSwish(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        x = self.custom_activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def objective_function(parameters, activation, train_loader, test_loader, device):\n",
    "    if activation in ['SLA', 'R3elu']:\n",
    "        hyperparams = {\n",
    "            'C': float(parameters[:, 0]),\n",
    "            'K': int(parameters[:, 1]),\n",
    "            'epsilon_p': float(parameters[:, 2]),\n",
    "            'epsilon_l': float(parameters[:, 3]),\n",
    "            'probability': float(parameters[:, 4])\n",
    "        }\n",
    "    elif activation == 'RSwish':\n",
    "        hyperparams = {\n",
    "            'mean': float(parameters[:, 0]),\n",
    "            'stddev': float(parameters[:, 1])\n",
    "        }\n",
    "    elif activation == 'LapSwish':\n",
    "        hyperparams = {\n",
    "            'scale': float(parameters[:, 0]),\n",
    "            'epsilon_p': float(parameters[:, 1])\n",
    "        }\n",
    "    \n",
    "    model = FashionMNISTModel(activation=activation, **hyperparams)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    accuracy = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device)\n",
    "    \n",
    "    return -accuracy  # We want to maximize accuracy, but GPyOpt minimizes the objective function\n",
    "\n",
    "def bayesian_optimization(activation, train_loader, test_loader, device, max_iter=50):\n",
    "    if activation in ['SLA', 'R3elu']:\n",
    "        bounds = [\n",
    "            {'name': 'C', 'type': 'continuous', 'domain': (0.1, 2.0)},\n",
    "            {'name': 'K', 'type': 'discrete', 'domain': (1, 2, 3)},\n",
    "            {'name': 'epsilon_p', 'type': 'continuous', 'domain': (0.1, 10.0)},\n",
    "            {'name': 'epsilon_l', 'type': 'continuous', 'domain': (0.1, 1.0)},\n",
    "            {'name': 'probability', 'type': 'continuous', 'domain': (0.3, 0.7)}\n",
    "        ]\n",
    "    elif activation == 'RSwish':\n",
    "        bounds = [\n",
    "            {'name': 'mean', 'type': 'continuous', 'domain': (0.8, 1.2)},\n",
    "            {'name': 'stddev', 'type': 'continuous', 'domain': (0.05, 0.2)}\n",
    "        ]\n",
    "    elif activation == 'LapSwish':\n",
    "        bounds = [\n",
    "            {'name': 'scale', 'type': 'continuous', 'domain': (0.01, 0.5)},\n",
    "            {'name': 'epsilon_p', 'type': 'continuous', 'domain': (0.1, 10.0)}\n",
    "        ]\n",
    "    \n",
    "    optimizer = GPyOpt.methods.BayesianOptimization(\n",
    "        f=partial(objective_function, activation=activation, train_loader=train_loader, test_loader=test_loader, device=device),\n",
    "        domain=bounds,\n",
    "        model_type='GP',\n",
    "        acquisition_type='EI',\n",
    "        maximize=False,\n",
    "        verbosity=True\n",
    "    )\n",
    "    \n",
    "    optimizer.run_optimization(max_iter=max_iter)\n",
    "    \n",
    "    best_hyperparams = optimizer.x_opt\n",
    "    best_accuracy = -optimizer.fx_opt  # Remember we minimized negative accuracy\n",
    "    \n",
    "    return best_hyperparams, best_accuracy\n",
    "\n",
    "def parallel_bayesian_optimization(activation_functions, train_loader, test_loader, device, max_iter=50, num_processes=2):\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        results = pool.starmap(\n",
    "            bayesian_optimization,\n",
    "            [(activation, train_loader, test_loader, device, max_iter) for activation in activation_functions]\n",
    "        )\n",
    "    \n",
    "    best_activation = None\n",
    "    best_accuracy = 0\n",
    "    best_hyperparams = None\n",
    "    \n",
    "    for activation, (hyperparams, accuracy) in zip(activation_functions, results):\n",
    "        print(f'Activation: {activation}, Best Hyperparameters: {hyperparams}, Best Accuracy: {accuracy}')\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_activation = activation\n",
    "            best_hyperparams = hyperparams\n",
    "    \n",
    "    return best_activation, best_hyperparams, best_accuracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Prepare the dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Perform parallel Bayesian Optimization\n",
    "    activation_functions = ['SLA', 'R3elu', 'RSwish', 'LapSwish']\n",
    "    best_activation, best_hyperparams, best_accuracy = parallel_bayesian_optimization(\n",
    "        activation_functions, train_loader, test_loader, device, max_iter=50, num_processes=2\n",
    "    )\n",
    "    print(f'Overall Best Activation: {best_activation}, Best Hyperparameters: {best_hyperparams}, Best Accuracy: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import recall_score\n",
    "import GPyOpt\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define Stochastic Laplacian Activation (SLA)\n",
    "class StochasticLaplacianActivation(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(StochasticLaplacianActivation, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.clipK(x, self.C, self.K)\n",
    "        mask = torch.rand_like(x_hat) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=x_hat.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, torch.maximum(x_hat + laplacian_noise, torch.tensor(0.0).to(x.device)), x_hat)\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale\n",
    "\n",
    "# Define R3elu\n",
    "class R3elu(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(R3elu, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.clipK(x, self.C, self.K)\n",
    "        mask = torch.rand_like(x_hat) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=x_hat.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, F.relu(x_hat + laplacian_noise), F.relu(x_hat))\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale\n",
    "\n",
    "# Define Randomized Swish (RSwish)\n",
    "class RSwish(nn.Module):\n",
    "    def __init__(self, mean=1.0, stddev=0.1):\n",
    "        super(RSwish, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * torch.sigmoid(x)\n",
    "        noise = torch.normal(self.mean, self.stddev, size=swish.size()).to(x.device)\n",
    "        return swish * noise\n",
    "\n",
    "# Define Laplacian Swish (LapSwish)\n",
    "class LapSwish(nn.Module):\n",
    "    def __init__(self, scale=0.1, epsilon_p=1.0):\n",
    "        super(LapSwish, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.epsilon_p = epsilon_p\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * torch.sigmoid(x)\n",
    "        noise_scale = self.scale / self.epsilon_p\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=swish.size()), dtype=torch.float32).to(x.device)\n",
    "        return swish + laplacian_noise\n",
    "\n",
    "# Define the Fashion MNIST model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self, activation='SLA', **kwargs):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        if activation == 'SLA':\n",
    "            self.custom_activation = StochasticLaplacianActivation(**kwargs)\n",
    "        elif activation == 'R3elu':\n",
    "            self.custom_activation = R3elu(**kwargs)\n",
    "        elif activation == 'RSwish':\n",
    "            self.custom_activation = RSwish(**kwargs)\n",
    "        elif activation == 'LapSwish':\n",
    "            self.custom_activation = LapSwish(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        x = self.custom_activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def objective_function(parameters, activation, train_loader, test_loader, device):\n",
    "    if activation in ['SLA', 'R3elu']:\n",
    "        hyperparams = {\n",
    "            'C': float(parameters[:, 0]),\n",
    "            'K': int(parameters[:, 1]),\n",
    "            'epsilon_p': float(parameters[:, 2]),\n",
    "            'epsilon_l': float(parameters[:, 3]),\n",
    "            'probability': float(parameters[:, 4])\n",
    "        }\n",
    "    elif activation == 'RSwish':\n",
    "        hyperparams = {\n",
    "            'mean': float(parameters[:, 0]),\n",
    "            'stddev': float(parameters[:, 1])\n",
    "        }\n",
    "    elif activation == 'LapSwish':\n",
    "        hyperparams = {\n",
    "            'scale': float(parameters[:, 0]),\n",
    "            'epsilon_p': float(parameters[:, 1])\n",
    "        }\n",
    "    \n",
    "    model = FashionMNISTModel(activation=activation, **hyperparams)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    accuracy = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device)\n",
    "    \n",
    "    return -accuracy  # We want to maximize accuracy, but GPyOpt minimizes the objective function\n",
    "\n",
    "def bayesian_optimization(activation, train_loader, test_loader, device, max_iter=50):\n",
    "    if activation in ['SLA', 'R3elu']:\n",
    "        bounds = [\n",
    "            {'name': 'C', 'type': 'continuous', 'domain': (0.1, 2.0)},\n",
    "            {'name': 'K', 'type': 'discrete', 'domain': (1, 2, 3)},\n",
    "            {'name': 'epsilon_p', 'type': 'continuous', 'domain': (0.1, 10.0)},\n",
    "            {'name': 'epsilon_l', 'type': 'continuous', 'domain': (0.1, 1.0)},\n",
    "            {'name': 'probability', 'type': 'continuous', 'domain': (0.3, 0.7)}\n",
    "        ]\n",
    "    elif activation == 'RSwish':\n",
    "        bounds = [\n",
    "            {'name': 'mean', 'type': 'continuous', 'domain': (0.8, 1.2)},\n",
    "            {'name': 'stddev', 'type': 'continuous', 'domain': (0.05, 0.2)}\n",
    "        ]\n",
    "    elif activation == 'LapSwish':\n",
    "        bounds = [\n",
    "            {'name': 'scale', 'type': 'continuous', 'domain': (0.01, 0.5)},\n",
    "            {'name': 'epsilon_p', 'type': 'continuous', 'domain': (0.1, 10.0)}\n",
    "        ]\n",
    "    \n",
    "    optimizer = GPyOpt.methods.BayesianOptimization(\n",
    "        f=partial(objective_function, activation=activation, train_loader=train_loader, test_loader=test_loader, device=device),\n",
    "        domain=bounds,\n",
    "        model_type='GP',\n",
    "        acquisition_type='EI',\n",
    "        maximize=False,\n",
    "        verbosity=True\n",
    "    )\n",
    "    \n",
    "    optimizer.run_optimization(max_iter=max_iter)\n",
    "    \n",
    "    best_hyperparams = optimizer.x_opt\n",
    "    best_accuracy = -optimizer.fx_opt  # Remember we minimized negative accuracy\n",
    "    \n",
    "    return best_hyperparams, best_accuracy\n",
    "\n",
    "def parallel_bayesian_optimization(activation_functions, train_loader, test_loader, device, max_iter=50, num_processes=2):\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        results = pool.starmap(\n",
    "            bayesian_optimization,\n",
    "            [(activation, train_loader, test_loader, device, max_iter) for activation in activation_functions]\n",
    "        )\n",
    "    \n",
    "    best_activation = None\n",
    "    best_accuracy = 0\n",
    "    best_hyperparams = None\n",
    "    \n",
    "    for activation, (hyperparams, accuracy) in zip(activation_functions, results):\n",
    "        print(f'Activation: {activation}, Best Hyperparameters: {hyperparams}, Best Accuracy: {accuracy}')\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_activation = activation\n",
    "            best_hyperparams = hyperparams\n",
    "    \n",
    "    return best_activation, best_hyperparams, best_accuracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Prepare the dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Perform parallel Bayesian Optimization\n",
    "    activation_functions = ['SLA', 'R3elu', 'RSwish', 'LapSwish']\n",
    "    best_activation, best_hyperparams, best_accuracy = parallel_bayesian_optimization(\n",
    "        activation_functions, train_loader, test_loader, device, max_iter=50, num_processes=2\n",
    "    )\n",
    "    print(f'Overall Best Activation: {best_activation}, Best Hyperparameters: {best_hyperparams}, Best Accuracy: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esrav\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training and evaluating SLA...\n",
      "Activation: SLA, Accuracy: 0.8936\n",
      "\n",
      "Training and evaluating R3elu...\n",
      "Activation: R3elu, Accuracy: 0.8698\n",
      "\n",
      "Training and evaluating RSwish...\n",
      "Activation: RSwish, Accuracy: 0.9197\n",
      "\n",
      "Training and evaluating LapSwish...\n",
      "Activation: LapSwish, Accuracy: 0.9214\n",
      "\n",
      "Measuring randomness scores...\n",
      "Activation: SLA, Randomness Score: 0.018274\n",
      "Activation: R3elu, Randomness Score: 0.015372\n",
      "Activation: RSwish, Randomness Score: 0.000006\n",
      "Activation: LapSwish, Randomness Score: 0.000026\n",
      "Execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define Stochastic Laplacian Activation (SLA)\n",
    "class StochasticLaplacianActivation(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(StochasticLaplacianActivation, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.clipK(x, self.C, self.K)\n",
    "        mask = torch.rand_like(x_hat) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=x_hat.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, torch.maximum(x_hat + laplacian_noise, torch.tensor(0.0).to(x.device)), x_hat)\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale\n",
    "\n",
    "# Define R3elu\n",
    "class R3elu(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(R3elu, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.clipK(x, self.C, self.K)\n",
    "        mask = torch.rand_like(x_hat) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=x_hat.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, F.relu(x_hat + laplacian_noise), F.relu(x_hat))\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale\n",
    "\n",
    "# Define Randomized Swish (RSwish)\n",
    "class RSwish(nn.Module):\n",
    "    def __init__(self, mean=1.0, stddev=0.1):\n",
    "        super(RSwish, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * torch.sigmoid(x)\n",
    "        noise = torch.normal(self.mean, self.stddev, size=swish.size()).to(x.device)\n",
    "        return swish * noise\n",
    "\n",
    "# Define Laplacian Swish (LapSwish)\n",
    "class LapSwish(nn.Module):\n",
    "    def __init__(self, scale=0.1, epsilon_p=1.0):\n",
    "        super(LapSwish, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.epsilon_p = epsilon_p\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * torch.sigmoid(x)\n",
    "        noise_scale = self.scale / self.epsilon_p\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=swish.size()), dtype=torch.float32).to(x.device)\n",
    "        return swish + laplacian_noise\n",
    "\n",
    "# Define the Fashion MNIST model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self, activation='SLA', **kwargs):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        if activation == 'SLA':\n",
    "            self.custom_activation = StochasticLaplacianActivation(**kwargs)\n",
    "        elif activation == 'R3elu':\n",
    "            self.custom_activation = R3elu(**kwargs)\n",
    "        elif activation == 'RSwish':\n",
    "            self.custom_activation = RSwish(**kwargs)\n",
    "        elif activation == 'LapSwish':\n",
    "            self.custom_activation = LapSwish(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        x = self.custom_activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def measure_randomness(activation, hyperparams, device, num_runs=100):\n",
    "    model = FashionMNISTModel(activation=activation, **hyperparams).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare a small batch of data\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Run the same input through the model multiple times\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            output = model(inputs)\n",
    "            outputs.append(output.cpu().numpy())\n",
    "\n",
    "    # Calculate the variance of the outputs\n",
    "    outputs = np.array(outputs)\n",
    "    variance = np.var(outputs, axis=0)\n",
    "    mean_variance = np.mean(variance)\n",
    "\n",
    "    return mean_variance\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Prepare the dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Best hyperparameters from the previous optimization\n",
    "    best_configs = {\n",
    "        'SLA': {'C': 0.74748618, 'K': 3, 'epsilon_p': 10.0, 'epsilon_l': 1.0, 'probability': 0.3},\n",
    "        'R3elu': {'C': 0.70169821, 'K': 3, 'epsilon_p': 10.0, 'epsilon_l': 1.0, 'probability': 0.3},\n",
    "        'RSwish': {'mean': 0.87115326, 'stddev': 0.0608454},\n",
    "        'LapSwish': {'scale': 0.01106899, 'epsilon_p': 1.74217515}\n",
    "    }\n",
    "\n",
    "    # Train and evaluate models with fixed parameters\n",
    "    for activation, hyperparams in best_configs.items():\n",
    "        print(f\"\\nTraining and evaluating {activation}...\")\n",
    "        model = FashionMNISTModel(activation=activation, **hyperparams)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        accuracy = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device)\n",
    "        print(f\"Activation: {activation}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Measure randomness\n",
    "    print(\"\\nMeasuring randomness scores...\")\n",
    "    for activation, hyperparams in best_configs.items():\n",
    "        randomness_score = measure_randomness(activation, hyperparams, device)\n",
    "        print(f\"Activation: {activation}, Randomness Score: {randomness_score:.6f}\")\n",
    "\n",
    "print(\"Execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training SLA model...\n",
      "Epoch 1/10 completed.\n",
      "Epoch 2/10 completed.\n",
      "Epoch 3/10 completed.\n",
      "Epoch 4/10 completed.\n",
      "Epoch 5/10 completed.\n",
      "Epoch 6/10 completed.\n",
      "Epoch 7/10 completed.\n",
      "Epoch 8/10 completed.\n",
      "Epoch 9/10 completed.\n",
      "Epoch 10/10 completed.\n",
      "Training completed.\n",
      "Training Swish model...\n",
      "Epoch 1/10 completed.\n",
      "Epoch 2/10 completed.\n",
      "Epoch 3/10 completed.\n",
      "Epoch 4/10 completed.\n",
      "Epoch 5/10 completed.\n",
      "Epoch 6/10 completed.\n",
      "Epoch 7/10 completed.\n",
      "Epoch 8/10 completed.\n",
      "Epoch 9/10 completed.\n",
      "Epoch 10/10 completed.\n",
      "Training completed.\n",
      "Visualization saved as 'sla_swish_comparison.png'\n",
      "Execution completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define Stochastic Laplacian Activation (SLA)\n",
    "class StochasticLaplacianActivation(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(StochasticLaplacianActivation, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.clipK(x, self.C, self.K)\n",
    "        mask = torch.rand_like(x_hat) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=x_hat.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, torch.maximum(x_hat + laplacian_noise, torch.tensor(0.0).to(x.device)), x_hat)\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale\n",
    "\n",
    "# Define the Fashion MNIST model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self, activation='SLA', **kwargs):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        if activation == 'SLA':\n",
    "            self.custom_activation = StochasticLaplacianActivation(**kwargs)\n",
    "        elif activation == 'Swish':\n",
    "            self.custom_activation = nn.SiLU()  # Swish activation\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        x = self.custom_activation(x)\n",
    "        features = x  # Save the features after activation\n",
    "        x = self.fc2(x)\n",
    "        return x, features\n",
    "\n",
    "# Simple generator\n",
    "class SimpleGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleGenerator, self).__init__()\n",
    "        self.fc = nn.Linear(128, 28 * 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train_model(model, train_loader, device, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "\n",
    "def visualize_activations(sla_model, swish_model, generator, test_loader, device, num_samples=5, num_generations=3):\n",
    "    sla_model.eval()\n",
    "    swish_model.eval()\n",
    "    generator.eval()\n",
    "    \n",
    "    # Get some test images\n",
    "    dataiter = iter(test_loader)\n",
    "    images, _ = next(dataiter)\n",
    "    images = images[:num_samples].to(device)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    for i in range(num_samples):\n",
    "        # Display original image\n",
    "        plt.subplot(num_samples, 2*num_generations + 1, i*(2*num_generations + 1) + 1)\n",
    "        plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Original')\n",
    "        \n",
    "        # Generate images using SLA model\n",
    "        for j in range(num_generations):\n",
    "            with torch.no_grad():\n",
    "                _, features = sla_model(images[i].unsqueeze(0))\n",
    "                generated = generator(features)\n",
    "            \n",
    "            plt.subplot(num_samples, 2*num_generations + 1, i*(2*num_generations + 1) + j + 2)\n",
    "            plt.imshow(generated.cpu().squeeze(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            if i == 0:\n",
    "                plt.title(f'SLA Gen {j+1}')\n",
    "\n",
    "        # Generate images using Swish model\n",
    "        for j in range(num_generations):\n",
    "            with torch.no_grad():\n",
    "                _, features = swish_model(images[i].unsqueeze(0))\n",
    "                generated = generator(features)\n",
    "            \n",
    "            plt.subplot(num_samples, 2*num_generations + 1, i*(2*num_generations + 1) + num_generations + j + 2)\n",
    "            plt.imshow(generated.cpu().squeeze(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            if i == 0:\n",
    "                plt.title(f'Swish Gen {j+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sla_swish_comparison.png')\n",
    "    print(\"Visualization saved as 'sla_swish_comparison.png'\")\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up device and data loaders\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Initialize and train the SLA model\n",
    "    sla_params = {'C': 0.74748618, 'K': 3, 'epsilon_p': 10.0, 'epsilon_l': 1.0, 'probability': 0.3}\n",
    "    sla_model = FashionMNISTModel(activation='SLA', **sla_params).to(device)\n",
    "    print(\"Training SLA model...\")\n",
    "    train_model(sla_model, train_loader, device)\n",
    "\n",
    "    # Initialize and train the Swish model\n",
    "    swish_model = FashionMNISTModel(activation='Swish').to(device)\n",
    "    print(\"Training Swish model...\")\n",
    "    train_model(swish_model, train_loader, device)\n",
    "\n",
    "    # Initialize the generator\n",
    "    generator = SimpleGenerator().to(device)\n",
    "\n",
    "    # Visualize the effect of SLA vs Swish\n",
    "    visualize_activations(sla_model, swish_model, generator, test_loader, device)\n",
    "\n",
    "    print(\"Execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets finalize on SLA activation function and do some more tests\n",
    "\n",
    "The Stochastic Laplacian Activation (SLA) function is designed to introduce controlled noise into the activation process, enhancing privacy while maintaining model performance. Here's a breakdown of its key mathematical components:\n",
    "\n",
    "Clipping Function:\n",
    "The SLA first applies a clipping function to the input:\n",
    "x_hat = clipK(x, C, K)\n",
    "Where:\n",
    "\n",
    "x is the input\n",
    "C is a clipping parameter\n",
    "K is the norm order\n",
    "\n",
    "The clipK function is defined as:\n",
    "clipK(v, C, K) = v * min(1, C / ||v||_K)\n",
    "Where ||v||_K is the K-norm of v.\n",
    "Stochastic Noise Addition:\n",
    "After clipping, SLA adds Laplacian noise with probability p:\n",
    "output = {\n",
    "x_hat + Lap(0, 2KC/εℓ), with probability p\n",
    "x_hat,                  with probability 1-p\n",
    "}\n",
    "Where:\n",
    "\n",
    "Lap(0, 2KC/εℓ) is Laplacian noise with location 0 and scale 2KC/εℓ\n",
    "εℓ is the local privacy parameter\n",
    "p is the probability of adding noise\n",
    "\n",
    "\n",
    "Privacy Guarantees:\n",
    "SLA provides (ε, δ)-local differential privacy, where:\n",
    "ε = εℓ + ln(1/δ) / p\n",
    "This means that for any two possible inputs x and x', and any output y:\n",
    "Pr[SLA(x) = y] ≤ exp(ε) * Pr[SLA(x') = y] + δ\n",
    "Hyperparameters:\n",
    "The key hyperparameters in SLA are:\n",
    "\n",
    "C: clipping threshold\n",
    "K: norm order for clipping\n",
    "εℓ: local privacy parameter\n",
    "p: probability of adding noise\n",
    "\n",
    "\n",
    "Implementation Details:\n",
    "In our PyTorch implementation, we used:\n",
    "\n",
    "torch.norm for K-norm calculation\n",
    "torch.clamp for clipping\n",
    "np.random.laplace for generating Laplacian noise\n",
    "torch.rand for probabilistic noise addition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also testing new efficient activation to preserve privacy of members with \"ClippedstocasticSwish\"\n",
    "\n",
    "class ClippedStochasticSwish(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(ClippedStochasticSwish, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_clipped = self.clipK(x, self.C, self.K)\n",
    "        y = x_clipped * torch.sigmoid(x_clipped)\n",
    "        mask = torch.rand_like(y) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=y.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, y + laplacian_noise, y)\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########iter 2 where all the new suggestions like distance based correlation and more tests are added #####\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "#from sklearn.model_selection imp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class StochasticLaplacianActivation(nn.Module):\n",
    "    def __init__(self, C, K, epsilon_p, epsilon_l, probability):\n",
    "        super(StochasticLaplacianActivation, self).__init__()\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_l = epsilon_l\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.clipK(x, self.C, self.K)\n",
    "        mask = torch.rand_like(x_hat) < self.probability\n",
    "        noise_scale = 2 * self.K * self.C / (self.epsilon_l * self.epsilon_p)\n",
    "        laplacian_noise = torch.tensor(np.random.laplace(0, noise_scale, size=x_hat.shape), dtype=torch.float32).to(x.device)\n",
    "        output = torch.where(mask, torch.maximum(x_hat + laplacian_noise, torch.tensor(0.0).to(x.device)), x_hat)\n",
    "        return output\n",
    "\n",
    "    def clipK(self, v, C, K):\n",
    "        norm_v = torch.norm(v, p=K, dim=1, keepdim=True)\n",
    "        scale = torch.clamp(C / norm_v, max=1.0)\n",
    "        return v * scale\n",
    "\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self, activation='SLA', **kwargs):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        if activation == 'SLA':\n",
    "            self.custom_activation = StochasticLaplacianActivation(**kwargs)\n",
    "        elif activation == 'ReLU':\n",
    "            self.custom_activation = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        features = self.custom_activation(x)\n",
    "        x = self.fc2(features)\n",
    "        return x, features\n",
    "\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "class DeepPropertyInferenceAttack(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepPropertyInferenceAttack, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "def train_model_with_adversary(model, adversary, train_loader, device, num_epochs=10, lambda_adv=0.1):\n",
    "    model_optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    adv_optimizer = optim.Adam(adversary.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    adv_criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        adversary.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            adv_optimizer.zero_grad()\n",
    "            outputs, features = model(inputs)\n",
    "            adv_pred = adversary(features.detach())\n",
    "            adv_loss = adv_criterion(adv_pred, (labels == 0).float().unsqueeze(1))\n",
    "            adv_loss.backward()\n",
    "            adv_optimizer.step()\n",
    "            \n",
    "            model_optimizer.zero_grad()\n",
    "            outputs, features = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            adv_pred = adversary(features)\n",
    "            loss -= lambda_adv * adv_criterion(adv_pred, (labels == 0).float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "\n",
    "def distance_correlation_loss(X, Y):\n",
    "    def compute_distance_matrix(X):\n",
    "        return torch.cdist(X, X)\n",
    "\n",
    "    def compute_centered_distance_matrix(D):\n",
    "        n = D.size(0)\n",
    "        m = D.mean()\n",
    "        row_mean = D.mean(dim=1, keepdim=True)\n",
    "        col_mean = D.mean(dim=0, keepdim=True)\n",
    "        return D - row_mean - col_mean + m\n",
    "\n",
    "    n = X.size(0)\n",
    "    dX = compute_centered_distance_matrix(compute_distance_matrix(X))\n",
    "    dY = compute_centered_distance_matrix(compute_distance_matrix(Y))\n",
    "    dXY = torch.mul(dX, dY)\n",
    "    return dXY.sum() / (n * (n-3))\n",
    "\n",
    "def train_model_with_privacy(model, train_loader, device, num_epochs=10, lambda_dc=0.1):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, features = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            dc_loss = distance_correlation_loss(inputs.view(inputs.size(0), -1), features)\n",
    "            loss += lambda_dc * dc_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "              \n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()   \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def advanced_property_inference_attack(model, target_loader, shadow_loader, device):\n",
    "    attack_model = DeepPropertyInferenceAttack(10).to(device)\n",
    "    optimizer = optim.Adam(attack_model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for inputs, labels in target_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs, _ = model(inputs)\n",
    "            \n",
    "            attack_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred = attack_model(outputs)\n",
    "            loss = criterion(pred, (labels == 0).float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    attack_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in shadow_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            pred = attack_model(outputs)\n",
    "            correct += ((pred > 0.5) == (labels == 0).unsqueeze(1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def member_inference_attack(model, member_loader, non_member_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    def get_confidences(loader):\n",
    "        confidences = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs, _ = model(inputs)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                confidences.extend(probs.max(dim=1)[0].cpu().numpy())\n",
    "        return confidences\n",
    "\n",
    "    member_confidences = get_confidences(member_loader)\n",
    "    non_member_confidences = get_confidences(non_member_loader)\n",
    "\n",
    "    threshold = np.mean(member_confidences)\n",
    "    member_preds = [1 if conf >= threshold else 0 for conf in member_confidences]\n",
    "    non_member_preds = [1 if conf >= threshold else 0 for conf in non_member_confidences]\n",
    "\n",
    "    member_accuracy = np.mean(member_preds)\n",
    "    non_member_accuracy = 1 - np.mean(non_member_preds)\n",
    "\n",
    "    attack_accuracy = (member_accuracy + non_member_accuracy) / 2\n",
    "    return attack_accuracy\n",
    "\n",
    "def data_reconstruction_attack(model, target_loader, device, num_iterations=1000):\n",
    "    model.eval()\n",
    "    target_input, target_label = next(iter(target_loader))\n",
    "    target_input = target_input.to(device)\n",
    "    target_label = target_label.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_output, _ = model(target_input)\n",
    "\n",
    "    reconstructed_input = torch.randn_like(target_input, requires_grad=True)\n",
    "    optimizer = optim.Adam([reconstructed_input], lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(reconstructed_input)\n",
    "        loss = criterion(output, target_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    reconstruction_error = torch.mean((reconstructed_input - target_input) ** 2).item()\n",
    "    return reconstruction_error\n",
    "\n",
    "def plot_results(sla_results, baseline_results):\n",
    "    labels = ['Accuracy', 'Prop Inf', 'Mem Inf', 'Recon Error']\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    rects1 = ax.bar(x - width/2, sla_results, width, label='SLA Model')\n",
    "    rects2 = ax.bar(x + width/2, baseline_results, width, label='Baseline Model')\n",
    "\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Comparison of SLA and Baseline Models')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.bar_label(rects1, padding=3)\n",
    "    ax.bar_label(rects2, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    print(\"Comparison plot saved as 'model_comparison.png'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    full_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    sla_params = {'C': 0.74748618, 'K': 3, 'epsilon_p': 10.0, 'epsilon_l': 1.0, 'probability': 0.3}\n",
    "    sla_model = FashionMNISTModel(activation='SLA', **sla_params).to(device)\n",
    "    baseline_model = FashionMNISTModel(activation='ReLU').to(device)\n",
    "    adversary = Adversary(128).to(device)\n",
    "\n",
    "    print(\"Training SLA model with adversary...\")\n",
    "    train_model_with_adversary(sla_model, adversary, train_loader, device)\n",
    "    print(\"Training baseline model with privacy...\")\n",
    "    train_model_with_privacy(baseline_model, train_loader, device)\n",
    "\n",
    "    print(\"Evaluating models...\")\n",
    "    sla_accuracy = evaluate_model(sla_model, test_loader, device)\n",
    "    baseline_accuracy = evaluate_model(baseline_model, test_loader, device)\n",
    "\n",
    "    sla_prop_inf = advanced_property_inference_attack(sla_model, val_loader, test_loader, device)\n",
    "    baseline_prop_inf = advanced_property_inference_attack(baseline_model, val_loader, test_loader, device)\n",
    "\n",
    "    sla_mem_inf = member_inference_attack(sla_model, train_loader, test_loader, device)\n",
    "    baseline_mem_inf = member_inference_attack(baseline_model, train_loader, test_loader, device)\n",
    "\n",
    "    sla_recon_error = data_reconstruction_attack(sla_model, test_loader, device)\n",
    "    baseline_recon_error = data_reconstruction_attack(baseline_model, test_loader, device)\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"SLA Model - Accuracy: {sla_accuracy:.4f}, Prop Inf: {sla_prop_inf:.4f}, Mem Inf: {sla_mem_inf:.4f}, Recon Error: {sla_recon_error:.4f}\")\n",
    "    print(f\"Baseline Model - Accuracy: {baseline_accuracy:.4f}, Prop Inf: {baseline_prop_inf:.4f}, Mem Inf: {baseline_mem_inf:.4f}, Recon Error: {baseline_recon_error:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
